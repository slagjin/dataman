---
layout: post
title: "pyspark1"
categories: 语言工具
tags: [spark]
author:
  - Slags jin
---

* 编程模型  
    1. 初始化sparksession
    ```
         SparkContext，SparkConf
        在Spark2.0之前， SparkContext 是所有 Spark 功能的结构， 驱动器（driver） 通过SparkContext 连接到集群 （通过resource manager）， 因为在2.0之前， RDD就是Spark的基础。
        
        如果需要建立SparkContext，则需要SparkConf，通过Conf来配置SparkContext的内容。
        
        from pyspark import SparkConf,SparkContext
         
        conf = SparkConf().setAppName("app").setMaster('local')
        sc = SparkContext(conf=conf)
        setAppName(), 是你的程序在集群上的名字
        
        setMaster(), 你的Spark运行的模式 ‘local'表示本地模式
        
         
        
        SparkSession
        在Spark2.0之后，Spark Session也是Spark 的一个入口， 为了引入dataframe和dataset的API， 同时保留了原来SparkContext的functionality， 如果想要使用 HIVE，SQL，Streaming的API， 就需要Spark Session作为入口。
        
        spark = SparkSession.builder.appName('testSQL')\
                            .config('spark.some.config.option','some-value')\
                            .getOrCreate()
        如果要使用SparkContext的API
        
        spark.sparkContext.uiWebUrl

    ```
    2. 创建RDD   
        1. 并行化一个集合   
            ```
            data = sc.parallelize(
                [('Amber', 22), ('Alfred', 23), ('Skye',4), ('Albert', 12), 
                 ('Amber', 9)])
            ```
        2. 读取文件
            ```
            data_from_file = sc.\
                textFile(
                    '/Users/drabast/Documents/PySpark_Data/VS14MORT.txt.gz'
             ```       
    3. 创建DF [df使用](https://blog.csdn.net/suzyu12345/article/details/79673473 "读写df")
        1. 读取各类文件 json|csv|mysql|parquet|hive
        ```
            # 读取example下面的parquet文件
            file=r"D:\apps\spark-2.2.0-bin-hadoop2.7\examples\src\main\resources\users.parquet"
            df=spark.read.parquet(file)
            df.show()
        ```
        2. 从hive读取
        ```
            # 如果已经配置spark连接hive的参数，可以直接读取hive数据
            spark = SparkSession \
                    .builder \
                    .enableHiveSupport() \      
                    .master("172.31.100.170:7077") \
                    .appName("my_first_app_name") \
                    .getOrCreate()
            
            df=spark.sql("select * from hive_tb_name")
            df.show()
          ```     
    4. rdd和df相互转化 [相互转换](https://blog.csdn.net/zhurui_idea/article/details/73090951 "简单使用")
        1. rdd 转换成df 
            ```
            注意：rdd 必须是二维数组[[]]才能转成df
            schema = StructType([StructField('name',StringType(),True)])
            spark.createDataFrame(rdd,schema=None, samplingRatio=None).show()
            
            # 相互转化实例
            data_from_file = sc.\
                textFile('C:/Users/wb.jinchao2018/Desktop/jinchao.txt')
            print(data_from_file.collect())
            
            # 转成 df.     rdd 必须是二维数组才能转成df
            data=data_from_file.map(lambda x: (x.split(',')[0],x.split(',')[1],x.split(',')[2]))
            schema = StructType([StructField('name',StringType(),True),
                                 StructField('age',StringType(),True),
                                 StructField('brithday',StringType(),True)
                                 ]
                                )
            # spark.createDataFrame(data,schema=None, samplingRatio=None).show()
            data_df=spark.createDataFrame(data,schema, samplingRatio=None)
            data_df.show()
            ```
        2. df 转换成rdd 
            ```
            df.rdd()
            ```
        
        