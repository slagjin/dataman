---
layout: post
title: "pyspark1"
categories: 语言工具
tags: [spark]
author:
  - Slags jin
---


### pysaprk读写hive数据
[pyspark_hive](https://blog.csdn.net/u011412768/article/details/93426353 "命令")

* 读Hive表数据
```
from pyspark.sql import HiveContext,SparkSession
 
_SPARK_HOST = "spark://spark-master:7077"
_APP_NAME = "test"
spark_session = SparkSession.builder.master(_SPARK_HOST).appName(_APP_NAME).getOrCreate()
 
hive_context= HiveContext(spark_session )
 
# 生成查询的SQL语句，这个跟hive的查询语句一样，所以也可以加where等条件语句
hive_database = "database1"
hive_table = "test"
hive_read = "select * from  {}.{}".format(hive_database, hive_table)
 
# 通过SQL语句在hive中查询的数据直接是dataframe的形式
read_df = hive_context.sql(hive_read)
--------------------- 
```
* 将数据写入hive表
    1. 通过SQL语句生成表
    ```
    from pyspark.sql import SparkSession, HiveContext
    _SPARK_HOST = "spark://spark-master:7077"
    _APP_NAME = "test"
     
    spark = SparkSession.builder.master(_SPARK_HOST).appName(_APP_NAME).getOrCreate()
     
    data = [
        (1,"3","145"),
        (1,"4","146"),
        (1,"5","25"),
        (1,"6","26"),
        (2,"32","32"),
        (2,"8","134"),
        (2,"8","134"),
        (2,"9","137")
    ]
    df = spark.createDataFrame(data, ['id', "test_id", 'camera_id'])
     method one，default是默认数据库的名字，write_test 是要写到default中数据表的名字
    df.registerTempTable('test_hive')
    sqlContext.sql("create table default.write_test select * from test_hive")
    ```
    2. saveastable的方式
    ```
    # method two
    # "overwrite"是重写表的模式，如果表存在，就覆盖掉原始数据，如果不存在就重新生成一张表
    #  mode("append")是在原有表的基础上进行添加数据
    df.write.format("hive").mode("overwrite").saveAsTable('default.write_test')
    ```